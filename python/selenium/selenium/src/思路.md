### 提前准备

- Chrome 浏览器 查看版本号
- [chromedriver](https://registry.npmmirror.com/binary.html?path=chromedriver/&spm=a2c6h.24755359.0.0.6d444dccLzKAgY) 查看版本号，需要和浏览器版本号一致
- [selenium](https://registry.npmmirror.com/binary.html?path=selenium/&spm=a2c6h.24755359.0.0.6d444dccLzKAgY) 地址 通过 `pip3 install selenium` 安装
- Python3 环境 自行安装。 windows 下载好安装包一路下一步，记得勾中添加环境变量

### 使用

原理

- selenium 控制 chromedriver 来驱动 Chrome 浏览器

```python
# 引入 webdriver
from selenium import webdriver
# 保存 chromedriver 路径
PATH = 'D:\softwares\chromedriver.exe'
# 接收 webdriver 驱动
driver = webdriver.Chrome(PATH)
# 获取目标地址
driver.get('https://tj.lianjia.com/ershoufang/jinnan/')
# 通过 xpath 获取目标文本
total_nums = driver.find_element_by_xpath('//*[@id="content"]/div[1]/div[2]/h2/span')
# 打印爬取目标的文本内容
print(total_nums.text)
# 退出驱动 节省内存
driver.quit()
```

### 爬取大量数据思路

- 查看目标网站信息 链家天津二手房举例
  - https://tj.lianjia.com/ershoufang/ 这是链家天津二手房所有房源
  - 分析
    - 总房源数据 十五万四千多
    - 按照目前分页来看，最多有 100 个分页，每页最多 30 条数据，如果按部就班爬取是爬不到十多万数据的，最多只能爬取 3000 条
  - 处理
    - 细分查询条件
      - 按照位置细分
        - 按照位置细分查看
          - https://tj.lianjia.com/ershoufang/heping/
          - https://tj.lianjia.com/ershoufang/nankai/
          - 。。。
          - https://tj.lianjia.com/ershoufang/ninghe/
          - 后面所有位置在最后的路径上替换对应位置的拼音
        - 搜索数据仍多余 3000 条
      - 在位置细分的基础上再增加搜索条件
        - 按售价细分
          - https://tj.lianjia.com/ershoufang/heping/p1/
          - https://tj.lianjia.com/ershoufang/heping/p2/
          - 。。。
          - https://tj.lianjia.com/ershoufang/heping/p7/
          - 按售价在 url 末尾增加 p1 - p7
      - 在以上基础下发现还有总数居多余 3000 的情况
        - 按房型细分
          - https://tj.lianjia.com/ershoufang/jinnan/l1p1/
          - https://tj.lianjia.com/ershoufang/jinnan/l2p1/
          - 。。。
          - https://tj.lianjia.com/ershoufang/jinnan/l6p1/
          - 房型 l1 -l6
      - 如果再有超过 3000 条数据，继续以此思路往下查找，此处不做过多拆解
    - 分页查询
      - 每一页不同的显示规律，最多到 100页
        - https://tj.lianjia.com/ershoufang/jinnan/l1l6p7/
        - https://tj.lianjia.com/ershoufang/jinnan/pg2l1l6p7/
        - https://tj.lianjia.com/ershoufang/jinnan/pg3l1l6p7/
        - 。。。
        - 分页 pg1 - pg100
    - 具体详情页查询
      - https://tj.lianjia.com/ershoufang/jinnan/l1l6p7/
        - 有 26 条数据
        - 每条数据的标题都带着具体的详情页 url
          - https://tj.lianjia.com/ershoufang/101113895984.html
          - https://tj.lianjia.com/ershoufang/101113405120.html
          - 。。。
      - 获取每一页的所有数据的详情页，即可获得所有房源的具体信息
    - 最终信息查询
      - 遍历所有详情页，去获取想要保存的数据
- 结果处理
  - 分析完毕，开始爬取数据

### 代码演示

- 安装 lxml 库
  - 这个库的主要优点是易于使用，在解析大型文档时速度非常快，归档的也非常好，并且提供了简单的转换方法来将数据转换为Python数据类型，从而使文件操作更容易。

```python
# 大量数据演示
city_url = 'https://tj.lianjia.com/ershoufang'
PATH = 'D:\softwares\chromedriver.exe'
driver = webdriver.Chrome(PATH)

# 获取具体位置的 url
def get_locate_url():
    range_urls = {}
    driver.get(city_url)
    html = driver.page_source
    selector = etree.HTML(html)
    for i in range(0, 19):
        a_href = str(selector.xpath('/html/body/div[3]/div/div[1]/dl[2]/dd/div[1]/div/a[' + str(i + 1) + ']/@href')[0])
        a_name = str(selector.xpath('/html/body/div[3]/div/div[1]/dl[2]/dd/div[1]/div/a[' + str(i + 1) + ']/text()')[0])
        range_url = 'https://tj.lianjia.com' + a_href
        range_urls[a_name] = range_url
    driver.quit()
    return range_urls
print(get_locate_url())

# 结果展示
{
  '和平': 'https://tj.lianjia.com/ershoufang/heping/',
  '南开': 'https://tj.lianjia.com/ershoufang/nankai/',
  '河西': 'https://tj.lianjia.com/ershoufang/hexi/',
  '河北': 'https://tj.lianjia.com/ershoufang/hebei/',
  '河东': 'https://tj.lianjia.com/ershoufang/hedong/',
  '红桥': 'https://tj.lianjia.com/ershoufang/hongqiao/',
  '西青': 'https://tj.lianjia.com/ershoufang/xiqing/',
  '北辰': 'https://tj.lianjia.com/ershoufang/beichen/',
  '东丽': 'https://tj.lianjia.com/ershoufang/dongli/',
  '津南': 'https://tj.lianjia.com/ershoufang/jinnan/',
  '塘沽': 'https://tj.lianjia.com/ershoufang/tanggu/',
  '开发区': 'https://tj.lianjia.com/ershoufang/kaifaqutj/',
  '武清': 'https://tj.lianjia.com/ershoufang/wuqing/',
  '滨海新区': 'https://tj.lianjia.com/ershoufang/binhaixinqu/',
  '宝坻': 'https://tj.lianjia.com/ershoufang/baodi/',
  '蓟州': 'https://tj.lianjia.com/ershoufang/jizhou/',
  '海河教育园区': 'https://tj.lianjia.com/ershoufang/haihejiaoyuyuanqu/',
  '静海': 'https://tj.lianjia.com/ershoufang/jinghai/',
  '宁河': 'https://tj.lianjia.com/ershoufang/ninghe/'
}

# 获取分条件查找的 url
def get_locate_detail_url():
    range_urls = get_locate_url()
    range_price_urls = {}
    for key in range_urls:
        range_url = range_urls[key]
        part_range_price_url = []
        total_urls = []
        for i in range(1, 7):
            range_price_url = range_url + 'l' + str(i)
            part_range_price_url.append(range_price_url)
        for i in part_range_price_url:
            for j in range(1, 8):
                range_price_url = i + 'p' + str(j)
                total_urls.append(range_price_url)
        range_price_urls[key] = total_urls
    return range_price_urls


print(get_locate_detail_url())
# 结果展示
{
  '和平': [
    'https://tj.lianjia.com/ershoufang/heping/l1p1',
    'https://tj.lianjia.com/ershoufang/heping/l1p2',
    'https://tj.lianjia.com/ershoufang/heping/l1p3',
    'https://tj.lianjia.com/ershoufang/heping/l1p4',
    'https://tj.lianjia.com/ershoufang/heping/l1p5',
    'https://tj.lianjia.com/ershoufang/heping/l1p6',
    'https://tj.lianjia.com/ershoufang/heping/l1p7',
    'https://tj.lianjia.com/ershoufang/heping/l2p1',
    'https://tj.lianjia.com/ershoufang/heping/l2p2',
    'https://tj.lianjia.com/ershoufang/heping/l2p3',
    'https://tj.lianjia.com/ershoufang/heping/l2p4',
    'https://tj.lianjia.com/ershoufang/heping/l2p5',
    'https://tj.lianjia.com/ershoufang/heping/l2p6',
    'https://tj.lianjia.com/ershoufang/heping/l2p7',
    'https://tj.lianjia.com/ershoufang/heping/l3p1',
    'https://tj.lianjia.com/ershoufang/heping/l3p2',
    'https://tj.lianjia.com/ershoufang/heping/l3p3',
    'https://tj.lianjia.com/ershoufang/heping/l3p4',
    'https://tj.lianjia.com/ershoufang/heping/l3p5',
    'https://tj.lianjia.com/ershoufang/heping/l3p6',
    'https://tj.lianjia.com/ershoufang/heping/l3p7',
    'https://tj.lianjia.com/ershoufang/heping/l4p1',
    'https://tj.lianjia.com/ershoufang/heping/l4p2',
    'https://tj.lianjia.com/ershoufang/heping/l4p3',
    'https://tj.lianjia.com/ershoufang/heping/l4p4',
    'https://tj.lianjia.com/ershoufang/heping/l4p5',
    'https://tj.lianjia.com/ershoufang/heping/l4p6',
    'https://tj.lianjia.com/ershoufang/heping/l4p7',
    'https://tj.lianjia.com/ershoufang/heping/l5p1',
    'https://tj.lianjia.com/ershoufang/heping/l5p2',
    'https://tj.lianjia.com/ershoufang/heping/l5p3',
    'https://tj.lianjia.com/ershoufang/heping/l5p4',
    'https://tj.lianjia.com/ershoufang/heping/l5p5',
    'https://tj.lianjia.com/ershoufang/heping/l5p6',
    'https://tj.lianjia.com/ershoufang/heping/l5p7',
    'https://tj.lianjia.com/ershoufang/heping/l6p1',
    'https://tj.lianjia.com/ershoufang/heping/l6p2',
    'https://tj.lianjia.com/ershoufang/heping/l6p3',
    'https://tj.lianjia.com/ershoufang/heping/l6p4',
    'https://tj.lianjia.com/ershoufang/heping/l6p5',
    'https://tj.lianjia.com/ershoufang/heping/l6p6',
    'https://tj.lianjia.com/ershoufang/heping/l6p7'
  ],
    ...
}

# 获取分页详情
def get_page_url():
    locate_detail_url = get_locate_detail_url()
    total_urls = []
    for key in locate_detail_url:
        locate_detail_urls = locate_detail_url[key]
        for url in locate_detail_urls:
            print(url)
            # 获取每一页有多少数据
            driver.get(url)
            # 等待资源加载完成
            time.sleep(2)
            html = driver.page_source
            selector = etree.HTML(html)
            house_nums = int(selector.xpath('//*[@id="content"]/div[1]/div[2]/h2/span/text()')[0])
            # 借助 python 整数类型特性
            # 每页最多 30 条
            # 如果取余为0 说明数据刚好够每页30条
            # 如果余数不为0 说明末尾页数据不足30 整体页数要 + 1
            yushu = house_nums % 30
            if yushu == 0:
                page_size = int(house_nums / 30)
            else:
                page_size = int(house_nums / 30) + 1
            print('数据量' + str(house_nums), '分页数' + str(page_size))
            for i in range(0, page_size):
                if i == 0:
                    last_url = url
                else:
                    # 'https://tj.lianjia.com/ershoufang/heping/l1p1'
                    # 分割结果 ['https:', '', 'tj.lianjia.com', 'ershoufang', 'heping', 'l1p1']
                    split_url = url.split('/')
                    url_detail_search = split_url[5]
                    # 字符串替换 完成最终目标 url 的拼接
                    last_url = url.replace(url_detail_search, 'pg' + str(i + 1) + url_detail_search)
                print(last_url)
                total_urls.append(last_url)
    return total_urls


print(get_page_url())
# 爬取完要很长时间 没有列出
# 模拟数据
# 获取具体详情页
def get_detail_urls():
    # total_urls = get_page_url()
    # 模拟获取 total_urls
    total_urls = ['https://tj.lianjia.com/ershoufang/heping/l1p1', 'https://tj.lianjia.com/ershoufang/heping/l1p2']
    for url in total_urls:
        driver.get(url)
        time.sleep(2)
        html = driver.page_source
        selector = etree.HTML(html)
        all_details = []
        for i in range(0, 30):
            try:
                # 数据量超过能爬到的数量就会报错
                # 报错即跳出循环读取下一页数据
                final_url = selector.xpath('//*[@id="content"]/div[1]/ul/li[' + str(i + 1) + ']/div[1]/div[1]/a/@href')[
                    0]
                all_details.append(final_url)
            except Exception:
                break
    return all_details


print(get_detail_urls())
# 结果
['https://tj.lianjia.com/ershoufang/101114569443.html','https://tj.lianjia.com/ershoufang/101114107583.html', 'https://tj.lianjia.com/ershoufang/101114422385.html', ...]

# 获取详细信息
def get_detail_info():
    detail_urls = get_detail_urls()
    all_details = []
    for url in detail_urls:
        driver.get(url)
        time.sleep(2)
        html = driver.page_source
        selector = etree.HTML(html)
        total_price = selector.xpath('//*[@class="total"]/text()')[0]
        unit_price = selector.xpath('//*[@class="unitPriceValue"]/text()')[0]
        apartment_name = selector.xpath('//*[@class="communityName"]/a[1]/text()')[0]
        range_area = selector.xpath('//*[@class="info"]/a[2]/text()')[0]
        all_details.append({'total_price': total_price, 'unit_price': unit_price, 'apartment_name': apartment_name,
                            'range_area': range_area})
    driver.quit()
    return all_details


print(get_detail_info())
# 结果
[{'total_price': '60', 'unit_price': '11851', 'apartment_name': '港湾中心', 'range_area': '小白楼'}, {'total_price': '70', 'unit_price': '8682', 'apartment_name': '万科世贸广场', 'range_area': '体育馆街'}, {'total_price': '68', 'unit_price': '16065', 'apartment_name': '天津中心', 'range_area': '南营门街'}]

```

### 思考

- `get_locate_detail_url` 方法自己拼出来的链接有的里面数据量为 0，后续可否优化
- 中断操作继续爬取数据时，如何避免对已爬取数据的重复爬取，避免做重复劳动
- 网络中断，或者其他情况下，如何进行已爬取数据的保存，避免浪费时间精力
- 爬取出错情况出现时，如何进行错误记录，方便后续操作
- 第一次全部数据爬取完毕后，如果记录了错误信息，如何快速进行数据的再次有效爬取